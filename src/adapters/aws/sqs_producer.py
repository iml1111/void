"""
SQS Producer Adapter

Handles sending messages to SQS queue (used by API)
"""
import json
import uuid
from typing import Dict, Any, Optional
from loguru import logger
from .sqs_client import SQSClient


class SQSProducerAdapter:
    """
    SQS message producer adapter

    Provides high-level interface for sending messages to SQS queue
    """

    def __init__(self, sqs_client: SQSClient, queue_url: str):
        """
        Initialize SQS producer

        Args:
            sqs_client: SQS client instance
            queue_url: SQS queue URL
        """
        self._client = sqs_client
        self._queue_url = queue_url

    def enqueue_task(
        self,
        task_type: str,
        data: Dict[str, Any],
        metadata: Optional[Dict[str, Any]] = None,
        message_group_id: Optional[str] = None
    ) -> str:
        """
        Enqueue task to SQS FIFO queue

        Args:
            task_type: Task type identifier
            data: Task data payload
            metadata: Additional metadata (optional)
            message_group_id: Message group ID (default: UUID for parallel processing)

        Note:
            - FIFO queues do not support DelaySeconds
            - MessageGroupId defaults to UUID for parallel processing
            - Pass explicit message_group_id to enforce sequential processing (e.g., per-user ordering)
            - MessageDeduplicationId is automatically generated by SQSClient
            - Each message gets unique deduplication ID regardless of content
        """
        try:
            # Build message body
            message_body = {
                'task_type': task_type,
                'data': data
            }

            if metadata:
                message_body['metadata'] = metadata

            # Default to UUID for parallel processing (or use explicit group_id for sequential)
            group_id = message_group_id or str(uuid.uuid4())

            # Send to SQS FIFO (deduplication ID auto-generated by SQSClient)
            response = self._client.send_message(
                queue_url=self._queue_url,
                message_body=json.dumps(message_body),
                message_group_id=group_id
            )

            message_id = response.get('MessageId')
            logger.info(
                f"Enqueued task: type={task_type}, message_id={message_id}, group={group_id}",
                extra={
                    "task_type": task_type,
                    "message_id": message_id,
                    "message_group_id": group_id
                }
            )

            return message_id

        except Exception as e:
            logger.error(f"Failed to enqueue task {task_type}: {e}")
            raise

    def enqueue_batch(
        self,
        tasks: list[Dict[str, Any]]
    ) -> list[str]:
        """
        Enqueue multiple tasks to FIFO queue (sequential for simplicity)

        Args:
            tasks: List of tasks
                [{
                    'task_type': 'xxx',
                    'data': {...},
                    'metadata': {...} (optional),
                    'message_group_id': 'xxx' (optional)
                }]
        """
        message_ids = []

        for task in tasks:
            message_id = self.enqueue_task(
                task_type=task['task_type'],
                data=task['data'],
                metadata=task.get('metadata'),
                message_group_id=task.get('message_group_id')
            )
            message_ids.append(message_id)

        logger.info(f"Enqueued {len(message_ids)} tasks")
        return message_ids
